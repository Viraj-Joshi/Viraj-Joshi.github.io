---
layout: post
title: "What is a score function?"
date: 2025-10-01
---

This post will serve two functions: to explain what a score function is, and to demonstrate the basics of JAX.

A score function is the gradient of the log probability density function with respect to the input.

$$ S(x) = \nabla_x \log p(x) $$

Surprisingly, [Hyv√§rinen et al.](http://jmlr.org/papers/v6/hyvarinen05a.html) showed we can estimate this function without knowing the density function $p(x)$ itself.

## How do we estimate it?

If we had the density, we could minimize the L2 loss between the true score function and our estimated score function $s_\theta(x)$ parameterized by $\theta$:

$$ \begin{align}
L(\theta) &= \frac{1}{2} \mathbb E_{x\sim p}\bigg[ || s_\theta(x) - \nabla_x \log p(x) ||^2_2 \bigg] \\
\end{align} $$

However, since we don't have the density , we can do some clever manipulation to remove this dependency.
$$
\begin{align*}
L(\theta) &= \frac{1}{2} \int p(x) || s_\theta(x) ||^2 dx - \underbrace{\int p(x) s_\theta(x)^T \nabla_x \log p(x) dx}_{\text{expanding log term}} + \frac{1}{2} \int p(x) || \nabla_x \log p(x) ||^2 dx \\
& \qquad  \qquad \qquad \qquad \qquad \qquad \qquad \qquad \Large\downarrow \\
&= \frac{1}{2} \int p(x) || s_\theta(x) ||^2 dx - 
\underbrace{\left( \int \cancel {p(x)} s_\theta(x)^T \frac{\nabla_x p(x)}{\cancel{p(x)}}dx\right)}_{\text{multi-dimensional IBP where u= } s_\theta(x)^T, dv = \nabla_x p(x) dx}
+\frac{1}{2} \int p(x) || \nabla_x \log p(x) ||^2 dx \\
& \qquad  \qquad \qquad \qquad \qquad \qquad \qquad \qquad \Large\downarrow \\
&= \frac{1}{2} \int p(x) || s_\theta(x) ||^2 dx -
\left( \underbrace{s_\theta(x)^T p(x)\big|_{-\infty}^{\infty}}_{\text{boundary term}} - \int p(x) (\nabla_x \cdot s_\theta(x))dx\right)+
\underbrace{\frac{1}{2} \int p(x) || \nabla_x \log p(x) ||^2 dx}_{\text{constant w.r.t. } \theta} \\
&= \mathbb E_{x\sim p}\bigg[ \frac{1}{2} || s_\theta(x) ||^2 + Tr(\nabla_x s_\theta(x)) \bigg] + C
\end{align*}
$$

Great, now we can approximate the expectation with samples from $p(x)$ and minimize the loss using gradient descent! JAX is new to me, so the below is mainly about JAX, and future posts will be about the difficulties of score matching, which motivates diffusion models.

## Our dataset

<div class="code-highlight">
<pre><code>
import jax
import jax.numpy as jnp
from jax import random, numpy as np, lax, vmap
import optax

# create a mixture of gaussians
def create_dataset(mus,sigmas,ws,n_samples=20000):
  keys = jax.random.split(key, len(mus) + 1)
  samples = []
  for mu, sigma, w, current_key in zip(mus, sigmas, ws, keys):
    num_samples = int(n_samples * w)
    samples.append(random.normal(current_key, shape=(num_samples,)) * sigma + mu)
  return jnp.concatenate(samples)

mus = [-2,2]
sigmas = [1,1]
ws = [.5,.5]
dataset = create_dataset(mus,sigmas,ws)
</code>
</pre>
</div>

We have a created a simple 1D equal mixture of Gaussians. 

The important part to note is that JAX uses pure functions, which means

1. For the same input, the function will always return the same output
2. The function has no side effects (it doesn't modify any external state)

In this case, this poses a problem for random generation, since if we call random.normal with the same key for each Gaussian, we will get the same samples every time.
Instead, we have to explicitly get 'mu' different keys by 'jax.random.split' because that is how many Gaussians we have.

<button class="dropdown-btn" onclick="toggleDropdown('code1')"></button> <span class="dropdown-label">code</span>
<div id="code1" class="dropdown-content">
<div class="code-highlight">
<pre>
<code>
import matplotlib.pyplot as plt
plt.hist(dataset,bins=100)
plt.show()
</code>
</pre>
</div>
</div>

## Creating our 1-layer MLP

<div class="code-highlight">
<pre><code>
szs = [1,1024]
out_dim = 1
szs = szs + [out_dim]
def create_mlp(szs):
  keys = random.split(key,len(szs))
  params = []
  for in_sz, out_sz, cur_key in zip(szs[:-1],szs[1:],keys):
    w_key, b_key = random.split(cur_key,2)
    w = random.normal(w_key,(in_sz,out_sz))
    b = random.normal(b_key,(out_sz,))
    params.append((w,b))
  return params

params = create_mlp(szs)

def relu(x):
  return jnp.maximum(0, x)

def forward(params, x):
  """
  params: list of weights and biases
  x: a single example of shape [feature_sz, ]
  """
  out = x
  for w,b in params[:-1]:
    out = jnp.dot(out,w) + b
    out = jax.nn.softplus(out)

  # output layer
  final_w, final_b = params[-1]
  out = jnp.dot(out,final_w) + final_b
  return out
</code>
</pre>
</div>

## One way to calculate gradients from a mini-batch

<div class="code-highlight">
<pre>
<code>
def score_matching_loss(params, x):
  """
  model: list of weights and biases
  x: shape [feature_sz,]
  """
  s_x = lambda x : forward(params, x)
  trace_term = jnp.trace(jax.jacfwd(s_x)(x))
  norm_term = .5 * jnp.sum(s_x(x))**2
  return trace_term + norm_term

model_to_loss_and_grad = jax.value_and_grad(score_matching_loss,argnums=0)

@jax.jit
def make_step(params, opt_state, batch):
  losses, grads = jax.vmap(model_to_loss_and_grad,in_axes=(None,0))(params, batch)
  # inaxes indicates to parallelize over the batch and use the same params for each

  loss = jnp.mean(losses)
  grad = jax.tree_util.tree_map(lambda g: jnp.mean(g, axis=0), grads)

  updates, opt_state = optimizer.update(grad, opt_state)
  params = optax.apply_updates(params, updates)
  return loss, params, opt_state
</code>
</pre>
</div>

### The better way

<div class="code-highlight">
<pre>
<code>
def score_matching_loss(params, x):
  """
  model: list of weights and biases
  x: shape [feature_sz,]
  """
  s_x = lambda x : forward(params, x)
  trace_term = jnp.trace(jax.jacfwd(s_x)(x))
  norm_term = .5 * jnp.sum(s_x(x))**2
  return trace_term + norm_term

def batch_loss(params, batch):
  # inaxes indicates to parallelize loss calculation over the batch and use the same params for each
  return jnp.mean(jax.vmap(score_matching_loss,in_axes=(None,0))(params, batch))

@jax.jit
def make_step(params, opt_state, batch):
  loss, grad = jax.value_and_grad(batch_loss)(params, batch)

  updates, opt_state = optimizer.update(grad, opt_state)
  params = optax.apply_updates(params, updates)
  return loss, params, opt_state
</code>
</pre>
</div>

## Training loop
<div class="code-highlight">
<pre>
<code>
learning_rate = 5e-3
epochs = 500
batch_size = 2048
optimizer = optax.adam(learning_rate)
opt_state = optimizer.init(params)

model_key, train_key = random.split(key,2)
num_batches = len(dataset) // batch_size
losses = []
for epoch in range(epochs):
  mb_losses = []
  for i in range(num_batches):
    train_key, choice_key, step_key = random.split(train_key,3)
    indices = random.choice(choice_key,len(dataset),shape=(batch_size,),replace=False)
    batch = dataset[indices]

    loss, params, opt_state = make_step(params, opt_state, batch)
    mb_losses.append(loss)
  loss = jnp.mean(jnp.array(mb_losses))
  losses.append(loss)
  print(f'Epoch: {epoch}, Loss: {loss}')
</code>
</pre>
</div>


<button class="dropdown-btn" onclick="toggleDropdown('code2')"></button> <span class="dropdown-label">plotting code</span>
<div id="code2" class="dropdown-content">
<div class="code-highlight">
<pre>
<code>
import matplotlib.pyplot as plt
plt.plot(losses)
plt.show()
</code>
</pre>
</div>
</div>

## True vs Expected Score
<div class="code-highlight">
<pre>
<code>
from jax import grad
from jax.scipy.stats import norm # Use JAX's SciPy stats for compatibility
import matplotlib.pyplot as plt


def mixture_norm_pdf(x, mus, sigmas, ws):
    mus = jnp.array(mus)
    sigmas = jnp.array(sigmas)
    ws = jnp.array(ws)

    pdf_values = ws * norm.pdf(x, loc=mus, scale=sigmas)

    return jnp.sum(pdf_values)

xs = jnp.arange(-4,4,.01)

## Calculate the true score function (gradient of log PDF)
def mixture_norm_log_pdf(x, mus, sigmas, ws):
    return jnp.log(mixture_norm_pdf(x, mus, sigmas, ws))

# Use jacfwd for scalar function and vmap for batching
true_score = vmap(jax.jacfwd(mixture_norm_log_pdf,argnums=0), in_axes=(0, None, None, None))
</code>
</pre>
</div>


<button class="dropdown-btn" onclick="toggleDropdown('code3')"></button> <span class="dropdown-label">plotting code</span>
<div id="code3" class="dropdown-content">
<div class="code-highlight">
<pre>
<code>
plt.plot(xs,true_score(xs, mus, sigmas, ws),label='true score')
plt.plot(xs,vmap(forward,in_axes=(None,0))(params,xs).squeeze(-1),label='learned score')

plt.legend()
plt.show()
</code>
</pre>
</div>
</div>

## How do we generate samples?

The scores tell us the direction of increasing likelihood, but that will result in deterministic sampling. As a result, we follow the 'noisy' scores. This is called Langevin dynamics. When run for long enough with small enough steps, x_t ~ p(x).

$$ x_{t+1} = x_t + \alpha_t \nabla_x \log p(x_t) + \sqrt{2\alpha} z_t, \quad z_t \sim \mathcal N(0, I) $$

Since we don't have the true score function, we will use our learned score function instead.

$$ x_{t+1} = x_t + \alpha_t s_\theta(x_t) + \sqrt{2\alpha} z_t, \quad z_t \sim \mathcal N(0, I) $$

<div class="code-highlight">
<pre>
<code>
alpha = 1e-2
n_particles = 10000
n_samples = 1000

def f(prev,key):
  epsilon = random.normal(key,shape=prev.shape)
  # Use in_axes=(None, 0) to indicate that params is not mapped (None)
  # and prev is mapped along its first axis (0)
  return prev + alpha*forward(params,prev) + np.sqrt(2 * alpha)*epsilon, prev

def g(x,key):
  keys = random.split(key,n_samples)
  # in jax, functions must be pure, for the same key, random.normal must always return the same number
  # so we create an array of n_samples keys so that each step of langevin sampling, we get a new epsilon
  res, history = lax.scan(f,init=x,xs=keys)
  return res, history


xs = random.uniform(key,shape=(n_particles,1,1))
keys = random.split(key,n_particles)
res, history = vmap(g)(xs,keys)
</code>
</pre>
</div>


<button class="dropdown-btn" onclick="toggleDropdown('code4')"></button> <span class="dropdown-label">plotting code</span>
<div id="code4" class="dropdown-content">
<div class="code-highlight">
<pre>
<code>
import matplotlib.pyplot as plt
plt.hist(res[:,0,0],bins=100)
plt.show()
</code>
</pre>
</div>
</div>
